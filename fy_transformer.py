# -*- coding: utf-8 -*-
"""FY_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krS8Ed8kC6BS5VOfME8R4hjGWf8gZBq-
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, TensorDataset
from transformers import BertModel, BertTokenizer, BertConfig
from sklearn.preprocessing import StandardScaler
import torch.optim as optim

# Load the SECOM dataset
secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'
secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'

# Load features and labels
secom_df = pd.read_csv(secom_data_url, sep=" ", header=None)
secom_labels = pd.read_csv(secom_labels_url, sep=" ", header=None)

# Preprocessing: handle missing values and scale data
# Replace 'NaN' values with the mean of the respective columns
secom_df.replace('NaN', np.nan, inplace=True)
secom_df = secom_df.fillna(secom_df.mean())

# Normalize data
scaler = StandardScaler()
secom_df = scaler.fit_transform(secom_df)

# Convert labels to binary
secom_labels.columns = ['Result', 'Timestamp']
secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(secom_df, secom_labels['Result'], test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Define a custom dataset
class SECOMDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define Transformer Model for classification
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead),
            num_layers=num_layers
        )
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(0)  # Add batch dimension for transformer input
        x = self.transformer(x)
        x = x.squeeze(0)  # Remove batch dimension
        x = self.fc(x)
        return x

# Model hyperparameters
input_dim = X_train.shape[1]  # 591 features
num_classes = 2  # Binary classification
hidden_dim = 256
nhead = 8
num_layers = 3

# Initialize model, loss function, and optimizer
model = TransformerModel(input_dim, num_classes, hidden_dim, nhead, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Function to train the model
def train_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')

# Function to evaluate the model
def evaluate_model(model, test_loader):
    model.eval()
    y_pred = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.cpu().numpy())
    return y_true, y_pred

# Train the model
train_model(model, train_loader, criterion, optimizer, epochs=10)

# Evaluate the model
y_true, y_pred = evaluate_model(model, test_loader)

# Print accuracy and classification report
print(f"Accuracy: {accuracy_score(y_true, y_pred)}")
print(classification_report(y_true, y_pred))

"""Add dropout to the transformer model to prevent overfitting.
Use KNN imputation for missing values.
Use PCA for feature reduction.
"""

from sklearn.impute import KNNImputer
from sklearn.decomposition import PCA

# Step 1: KNN Imputation for missing values
imputer = KNNImputer(n_neighbors=5)  # Use 5 nearest neighbors for imputation
secom_df_imputed = imputer.fit_transform(secom_df)

# Step 2: PCA for feature reduction
pca = PCA(n_components=100)  # Reduce to 100 components for now
secom_df_reduced = pca.fit_transform(secom_df_imputed)

# Update train-test split with reduced features
X_train, X_test, y_train, y_test = train_test_split(secom_df_reduced, secom_labels['Result'], test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Custom dataset and data loaders remain the same
train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Updated Transformer model with Dropout
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout),
            num_layers=num_layers
        )
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)
        x = self.dropout(x)  # Apply dropout after embedding
        x = x.unsqueeze(0)  # Add batch dimension for transformer input
        x = self.transformer(x)
        x = x.squeeze(0)  # Remove batch dimension
        x = self.fc(x)
        return x

# Initialize model with Dropout, optimizer, and loss function
model = TransformerModel(input_dim=X_train.shape[1], num_classes=2, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train and evaluate the model
train_model(model, train_loader, criterion, optimizer, epochs=10)
y_true, y_pred = evaluate_model(model, test_loader)

# Print accuracy and classification report
print(f"Accuracy: {accuracy_score(y_true, y_pred)}")
print(classification_report(y_true, y_pred))

"""Updated Code with SMOTE and Learning Rate Scheduler"""

from imblearn.over_sampling import SMOTE
import torch.optim.lr_scheduler as lr_scheduler

# Step 1: Apply SMOTE for class balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(secom_df_reduced, secom_labels['Result'])

# Step 2: Train-test split with balanced data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert to PyTorch tensors for both features and labels
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)

# Convert the labels to NumPy arrays before creating tensors
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Use .values to convert Series to NumPy array
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)    # Same for y_test

# Create dataset and dataloaders (same as before)
train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# Define model (same as previous with Dropout)
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout),
            num_layers=num_layers
        )
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)
        x = self.dropout(x)  # Apply dropout after embedding
        x = x.unsqueeze(0)  # Add batch dimension for transformer input
        x = self.transformer(x)
        x = x.squeeze(0)  # Remove batch dimension
        x = self.fc(x)
        return x

# Initialize model, criterion, optimizer, and scheduler
model = TransformerModel(input_dim=X_train.shape[1], num_classes=2, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Step 3: Use a learning rate scheduler to reduce the learning rate when the loss plateaus
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

# Training function with scheduler
def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Step the scheduler after each epoch based on the running loss
        scheduler.step(running_loss / len(train_loader))
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')

# Train the model
train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10)

# Evaluate the model
y_true, y_pred = evaluate_model(model, test_loader)

# Print accuracy and classification report
print(f"Accuracy: {accuracy_score(y_true, y_pred)}")
print(classification_report(y_true, y_pred))

"""Implementing PCA for Further Feature Reduction"""

from sklearn.decomposition import PCA

# Step 1: Apply PCA for dimensionality reduction
pca = PCA(n_components=100)  # Reducing to 100 components
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Convert to PyTorch tensors
X_train_tensor_pca = torch.tensor(X_train_pca, dtype=torch.float32)
X_test_tensor_pca = torch.tensor(X_test_pca, dtype=torch.float32)

# Keep labels the same
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Custom dataset and dataloaders for PCA reduced data
train_dataset_pca = SECOMDataset(X_train_tensor_pca, y_train_tensor)
test_dataset_pca = SECOMDataset(X_test_tensor_pca, y_test_tensor)

train_loader_pca = DataLoader(train_dataset_pca, batch_size=32, shuffle=True)
test_loader_pca = DataLoader(test_dataset_pca, batch_size=32, shuffle=False)

# Training the model (same as before)
train_model_with_scheduler(model, train_loader_pca, criterion, optimizer, scheduler, epochs=10)

# Evaluate the model
y_true_pca, y_pred_pca = evaluate_model(model, test_loader_pca)

# Print accuracy and classification report
print(f"Accuracy after PCA: {accuracy_score(y_true_pca, y_pred_pca)}")
print(classification_report(y_true_pca, y_pred_pca))

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler

# Load the SECOM dataset
secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'
secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'

# Load features and labels
secom_df = pd.read_csv(secom_data_url, sep=" ", header=None)
secom_labels = pd.read_csv(secom_labels_url, sep=" ", header=None)

# Preprocessing: handle missing values and scale data
secom_df.replace('NaN', np.nan, inplace=True)
imputer = KNNImputer(n_neighbors=5)
secom_df_imputed = imputer.fit_transform(secom_df)

# Normalize data
scaler = StandardScaler()
secom_df_scaled = scaler.fit_transform(secom_df_imputed)

# Convert labels to binary
secom_labels.columns = ['Result', 'Timestamp']
secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)

# Step 1: Apply PCA for feature reduction
pca = PCA(n_components=100)  # Reduce to 100 components for now
secom_df_reduced = pca.fit_transform(secom_df_scaled)

# Step 2: Apply SMOTE for class balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(secom_df_reduced, secom_labels['Result'])

# Train-test split with balanced data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert to PyTorch tensors for both features and labels
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Define a custom dataset
class SECOMDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define Transformer Model for classification with Dropout
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout),
            num_layers=num_layers)
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)
        x = self.dropout(x)  # Apply dropout after embedding
        x = x.unsqueeze(0)  # Add batch dimension for transformer input
        x = self.transformer(x)
        x = x.squeeze(0)  # Remove batch dimension
        x = self.fc(x)
        return x

# Initialize model, criterion, optimizer and scheduler
model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

# Function to train the model with scheduler
def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Step the scheduler after each epoch based on the running loss
        scheduler.step(running_loss / len(train_loader))
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')

# Function to evaluate the model
def evaluate_model(model, test_loader):
    model.eval()
    y_pred = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.cpu().numpy())
    return y_true, y_pred

# Train the model
train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10)

# Evaluate the model
y_true, y_pred = evaluate_model(model, test_loader)

# Print accuracy and classification report
print(f"Accuracy: {accuracy_score(y_true, y_pred)}")
print(classification_report(y_true, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler

# Load the SECOM dataset
secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'
secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'

# Load features and labels
secom_df = pd.read_csv(secom_data_url, sep=" ", header=None)
secom_labels = pd.read_csv(secom_labels_url, sep=" ", header=None)

# Preprocessing: handle missing values and scale data
imputer = KNNImputer(n_neighbors=5)
secom_df_imputed = imputer.fit_transform(secom_df)
scaler = StandardScaler()
secom_df_scaled = scaler.fit_transform(secom_df_imputed)

# Convert labels to binary
secom_labels.columns = ['Result', 'Timestamp']
secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)

# Step 1: Apply PCA for feature reduction
pca = PCA(n_components=100)
secom_df_reduced = pca.fit_transform(secom_df_scaled)

# Step 2: Apply SMOTE for class balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(secom_df_reduced, secom_labels['Result'])

# Train-test split with balanced data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert to PyTorch tensors for both features and labels
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Define a custom dataset
class SECOMDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define Transformer Model for classification with Dropout
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout) for _ in range(num_layers)
        ])
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, hidden_dim)
        x = self.dropout(x)    # Apply dropout after embedding

        # Reshape for transformer: (batch_size, seq_len (1), hidden_dim)
        x = x.unsqueeze(1)      # Add sequence length dimension

        for layer in self.transformer_layers:
            x = layer(x)         # Shape: (batch_size, seq_len (1), hidden_dim)

        x = x.squeeze(1)         # Remove sequence length dimension
        x = self.fc(x)           # Shape: (batch_size, num_classes)

        return x

# Initialize model with Dropout and optimizer
model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Learning rate scheduler setup
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

# Function to train the model with scheduler
def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Step the scheduler after each epoch based on the running loss
        scheduler.step(running_loss / len(train_loader))
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')

# Function to evaluate the model
def evaluate_model(model, test_loader):
    model.eval()
    y_pred = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.cpu().numpy())
    return y_true, y_pred

# Train the model with increased epochs and improved structure
train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20)

# Evaluate the model and print results
y_true, y_pred = evaluate_model(model, test_loader)
print(f"Accuracy: {accuracy_score(y_true, y_pred)}")
print(classification_report(y_true, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler

# Load the SECOM dataset
secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'
secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'

# Load features and labels
secom_df = pd.read_csv(secom_data_url, sep=" ", header=None)
secom_labels = pd.read_csv(secom_labels_url, sep=" ", header=None)

# Preprocessing: handle missing values and scale data
imputer = KNNImputer(n_neighbors=5)
secom_df_imputed = imputer.fit_transform(secom_df)
scaler = StandardScaler()
secom_df_scaled = scaler.fit_transform(secom_df_imputed)

# Convert labels to binary
secom_labels.columns = ['Result', 'Timestamp']
secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)

# Apply SMOTE for class balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(secom_df_scaled, secom_labels['Result'])

# Train-test split with balanced data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert to PyTorch tensors for both features and labels
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Define a custom dataset
class SECOMDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define Transformer Model for classification with Dropout and Batch Normalization
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout) for _ in range(num_layers)
        ])
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, hidden_dim)
        x = self.dropout(x)    # Apply dropout after embedding

        # Reshape for transformer: (batch_size, seq_len (1), hidden_dim)
        x = x.unsqueeze(1)      # Add sequence length dimension

        for layer in self.transformer_layers:
            x = layer(x)         # Shape: (batch_size, seq_len (1), hidden_dim)

        x = x.squeeze(1)         # Remove sequence length dimension
        x = self.fc(x)           # Shape: (batch_size, num_classes)

        return x

# Initialize model with Dropout and optimizer
model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Learning rate scheduler setup
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

# Function to train the model with scheduler
def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Step the scheduler after each epoch based on the running loss
        scheduler.step(running_loss / len(train_loader))
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')

# Function to evaluate the model
def evaluate_model(model, test_loader):
    model.eval()
    y_pred = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.cpu().numpy())
    return y_true, y_pred

# Train the model with increased epochs and improved structure
train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20)

# Evaluate the model and print results
y_true, y_pred = evaluate_model(model, test_loader)
print(f"Accuracy: {accuracy_score(y_true, y_pred)}")
print(classification_report(y_true, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from sklearn.decomposition import PCA
from sklearn.impute import KNNImputer

# Load the SECOM dataset
secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'
secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'

# Load features and labels
secom_df = pd.read_csv(secom_data_url, sep=" ", header=None)
secom_labels = pd.read_csv(secom_labels_url, sep=" ", header=None)

# Preprocessing: handle missing values and scale data
secom_df.replace('NaN', np.nan, inplace=True)
imputer = KNNImputer(n_neighbors=5)
secom_df_imputed = imputer.fit_transform(secom_df)

# Normalize data
scaler = StandardScaler()
secom_df_scaled = scaler.fit_transform(secom_df_imputed)

# Convert labels to binary
secom_labels.columns = ['Result', 'Timestamp']
secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)

# Step 1: Apply SMOTE for class balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(secom_df_scaled, secom_labels['Result'])

# Step 2: Train-test split with balanced data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert to PyTorch tensors for both features and labels
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Define a custom dataset
class SECOMDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)
test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define Transformer Model for classification with Dropout and Batch Normalization
class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout) for _ in range(num_layers)
        ])
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, hidden_dim)
        x = self.dropout(x)    # Apply dropout after embedding

        # Reshape for transformer: (batch_size, seq_len (1), hidden_dim)
        x = x.unsqueeze(1)      # Add sequence length dimension

        for layer in self.transformer_layers:
            x = layer(x)         # Shape: (batch_size, seq_len (1), hidden_dim)

        x = x.squeeze(1)         # Remove sequence length dimension
        x = self.fc(x)           # Shape: (batch_size, num_classes)

        return x

# Initialize model with Dropout and optimizer
model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Learning rate scheduler setup
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

# Function to train the model with scheduler
def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Step the scheduler after each epoch based on the running loss
        scheduler.step(running_loss / len(train_loader))
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')

# Function to evaluate the model
def evaluate_model(model, test_loader):
    model.eval()
    y_pred = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.cpu().numpy())
    return y_true, y_pred

# Train the model with increased epochs and improved structure
train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20)

# Evaluate the model and print results
y_true_pca , y_pred_pca  = evaluate_model(model,test_loader)
print(f"Accuracy after PCA: {accuracy_score(y_true_pca , y_pred_pca )}")
print(classification_report(y_true_pca , y_pred_pca ))