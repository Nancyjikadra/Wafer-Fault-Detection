{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HVB0OhP2Fz6"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oId-7ksuR55w"
      },
      "outputs": [],
      "source": [
        "# Load the SECOM dataset\n",
        "secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
        "secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
        "\n",
        "# Load features and labels\n",
        "secom_df = pd.read_csv(secom_data_url, sep=\" \", header=None)\n",
        "secom_labels = pd.read_csv(secom_labels_url, sep=\" \", header=None)\n",
        "\n",
        "# Preprocessing: handle missing values and scale data\n",
        "# Replace 'NaN' values with the mean of the respective columns\n",
        "secom_df.replace('NaN', np.nan, inplace=True)\n",
        "secom_df = secom_df.fillna(secom_df.mean())\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "secom_df = scaler.fit_transform(secom_df)\n",
        "\n",
        "# Convert labels to binary\n",
        "secom_labels.columns = ['Result', 'Timestamp']\n",
        "secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn8sqvkVR9gK"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(secom_df, secom_labels['Result'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpE1LMj6SBcU"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset\n",
        "class SECOMDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4bkXuS2SIXU",
        "outputId": "56b5ad16-da3c-450c-dbd7-27cd59f63351"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.28797928085550667\n",
            "Epoch 2/10, Loss: 0.22779787317849695\n",
            "Epoch 3/10, Loss: 0.2314736731350422\n",
            "Epoch 4/10, Loss: 0.23654514928348364\n",
            "Epoch 5/10, Loss: 0.23307902477681636\n",
            "Epoch 6/10, Loss: 0.2402514386922121\n",
            "Epoch 7/10, Loss: 0.24077847562730312\n",
            "Epoch 8/10, Loss: 0.24196320306509733\n",
            "Epoch 9/10, Loss: 0.2369678294286132\n",
            "Epoch 10/10, Loss: 0.24082234762609006\n",
            "Accuracy: 0.9235668789808917\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        24\n",
            "           1       0.92      1.00      0.96       290\n",
            "\n",
            "    accuracy                           0.92       314\n",
            "   macro avg       0.46      0.50      0.48       314\n",
            "weighted avg       0.85      0.92      0.89       314\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define Transformer Model for classification\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(0)  # Add batch dimension for transformer input\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # Remove batch dimension\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Model hyperparameters\n",
        "input_dim = X_train.shape[1]  # 591 features\n",
        "num_classes = 2  # Binary classification\n",
        "hidden_dim = 256\n",
        "nhead = 8\n",
        "num_layers = 3\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TransformerModel(input_dim, num_classes, hidden_dim, nhead, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "y_true, y_pred = evaluate_model(model, test_loader)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoG1ur_qTacQ"
      },
      "source": [
        "Add dropout to the transformer model to prevent overfitting.\n",
        "Use KNN imputation for missing values.\n",
        "Use PCA for feature reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbe5TZ6ATV15",
        "outputId": "eeda2e04-976c-4272-f155-515d96e3cf53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.3143065898140776\n",
            "Epoch 2/10, Loss: 0.2420719064772129\n",
            "Epoch 3/10, Loss: 0.2366992337629199\n",
            "Epoch 4/10, Loss: 0.2417006280273199\n",
            "Epoch 5/10, Loss: 0.23763134167529643\n",
            "Epoch 6/10, Loss: 0.24318651538342237\n",
            "Epoch 7/10, Loss: 0.23186061913147568\n",
            "Epoch 8/10, Loss: 0.24765461636707187\n",
            "Epoch 9/10, Loss: 0.25093495175242425\n",
            "Epoch 10/10, Loss: 0.23716117814183235\n",
            "Accuracy: 0.9235668789808917\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        24\n",
            "           1       0.92      1.00      0.96       290\n",
            "\n",
            "    accuracy                           0.92       314\n",
            "   macro avg       0.46      0.50      0.48       314\n",
            "weighted avg       0.85      0.92      0.89       314\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: KNN Imputation for missing values\n",
        "imputer = KNNImputer(n_neighbors=5)  # Use 5 nearest neighbors for imputation\n",
        "secom_df_imputed = imputer.fit_transform(secom_df)\n",
        "\n",
        "# Step 2: PCA for feature reduction\n",
        "pca = PCA(n_components=100)  # Reduce to 100 components for now\n",
        "secom_df_reduced = pca.fit_transform(secom_df_imputed)\n",
        "\n",
        "# Update train-test split with reduced features\n",
        "X_train, X_test, y_train, y_test = train_test_split(secom_df_reduced, secom_labels['Result'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Custom dataset and data loaders remain the same\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Updated Transformer model with Dropout\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)  # Apply dropout after embedding\n",
        "        x = x.unsqueeze(0)  # Add batch dimension for transformer input\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # Remove batch dimension\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model with Dropout, optimizer, and loss function\n",
        "model = TransformerModel(input_dim=X_train.shape[1], num_classes=2, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "y_true, y_pred = evaluate_model(model, test_loader)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1gif6vTs_W"
      },
      "source": [
        "Updated Code with SMOTE and Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAci0NHCTuCs",
        "outputId": "007e1a9a-2fa7-4cfc-ba4d-cedc12c7b66d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.5643275331806492\n",
            "Epoch 2/10, Loss: 0.4232871512303481\n",
            "Epoch 3/10, Loss: 0.30302662417493964\n",
            "Epoch 4/10, Loss: 0.23682577042160807\n",
            "Epoch 5/10, Loss: 0.1983211965274972\n",
            "Epoch 6/10, Loss: 0.21706474761201724\n",
            "Epoch 7/10, Loss: 0.19205411775289355\n",
            "Epoch 8/10, Loss: 0.1389718122176222\n",
            "Epoch 9/10, Loss: 0.13289408778419365\n",
            "Epoch 10/10, Loss: 0.13806186978881424\n",
            "Accuracy: 0.9505119453924915\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95       280\n",
            "           1       0.99      0.92      0.95       306\n",
            "\n",
            "    accuracy                           0.95       586\n",
            "   macro avg       0.95      0.95      0.95       586\n",
            "weighted avg       0.95      0.95      0.95       586\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Step 1: Apply SMOTE for class balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(secom_df_reduced, secom_labels['Result'])\n",
        "\n",
        "# Step 2: Train-test split with balanced data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors for both features and labels\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Convert the labels to NumPy arrays before creating tensors\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Use .values to convert Series to NumPy array\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)    # Same for y_test\n",
        "\n",
        "# Create dataset and dataloaders (same as before)\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Define model (same as previous with Dropout)\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)  # Apply dropout after embedding\n",
        "        x = x.unsqueeze(0)  # Add batch dimension for transformer input\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # Remove batch dimension\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, criterion, optimizer, and scheduler\n",
        "model = TransformerModel(input_dim=X_train.shape[1], num_classes=2, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 3: Use a learning rate scheduler to reduce the learning rate when the loss plateaus\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Training function with scheduler\n",
        "def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler after each epoch based on the running loss\n",
        "        scheduler.step(running_loss / len(train_loader))\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Train the model\n",
        "train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "y_true, y_pred = evaluate_model(model, test_loader)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzzW4rTXU89w"
      },
      "source": [
        "Implementing PCA for Further Feature Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHx4ODDWU2tx",
        "outputId": "d710e405-7b4c-49e1-8a5d-f14344fe0bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.732347804146844\n",
            "Epoch 2/10, Loss: 0.6309620476252323\n",
            "Epoch 3/10, Loss: 0.5462336113324037\n",
            "Epoch 4/10, Loss: 0.4713553133848551\n",
            "Epoch 5/10, Loss: 0.4364694417328448\n",
            "Epoch 6/10, Loss: 0.4287860329086716\n",
            "Epoch 7/10, Loss: 0.4053628968225943\n",
            "Epoch 8/10, Loss: 0.3980459983687143\n",
            "Epoch 9/10, Loss: 0.3830716400533109\n",
            "Epoch 10/10, Loss: 0.3795709873776178\n",
            "Accuracy after PCA: 0.8344709897610921\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       280\n",
            "           1       0.83      0.86      0.84       306\n",
            "\n",
            "    accuracy                           0.83       586\n",
            "   macro avg       0.84      0.83      0.83       586\n",
            "weighted avg       0.83      0.83      0.83       586\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=100)  # Reducing to 100 components\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor_pca = torch.tensor(X_train_pca, dtype=torch.float32)\n",
        "X_test_tensor_pca = torch.tensor(X_test_pca, dtype=torch.float32)\n",
        "\n",
        "# Keep labels the same\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Custom dataset and dataloaders for PCA reduced data\n",
        "train_dataset_pca = SECOMDataset(X_train_tensor_pca, y_train_tensor)\n",
        "test_dataset_pca = SECOMDataset(X_test_tensor_pca, y_test_tensor)\n",
        "\n",
        "train_loader_pca = DataLoader(train_dataset_pca, batch_size=32, shuffle=True)\n",
        "test_loader_pca = DataLoader(test_dataset_pca, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training the model (same as before)\n",
        "train_model_with_scheduler(model, train_loader_pca, criterion, optimizer, scheduler, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "y_true_pca, y_pred_pca = evaluate_model(model, test_loader_pca)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy after PCA: {accuracy_score(y_true_pca, y_pred_pca)}\")\n",
        "print(classification_report(y_true_pca, y_pred_pca))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Vi91mmopEh",
        "outputId": "7484ddbe-aff0-4040-f0d4-04b87af023d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.5647906240679927\n",
            "Epoch 2/10, Loss: 0.4266208688149581\n",
            "Epoch 3/10, Loss: 0.329221808427089\n",
            "Epoch 4/10, Loss: 0.31718335285581445\n",
            "Epoch 5/10, Loss: 0.23962807141848513\n",
            "Epoch 6/10, Loss: 0.19725953656676654\n",
            "Epoch 7/10, Loss: 0.17302653559710127\n",
            "Epoch 8/10, Loss: 0.15802848477520653\n",
            "Epoch 9/10, Loss: 0.1377283816439779\n",
            "Epoch 10/10, Loss: 0.10893618119125431\n",
            "Accuracy: 0.9436860068259386\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.94       280\n",
            "           1       0.98      0.91      0.94       306\n",
            "\n",
            "    accuracy                           0.94       586\n",
            "   macro avg       0.94      0.95      0.94       586\n",
            "weighted avg       0.95      0.94      0.94       586\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Load the SECOM dataset\n",
        "secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
        "secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
        "\n",
        "# Load features and labels\n",
        "secom_df = pd.read_csv(secom_data_url, sep=\" \", header=None)\n",
        "secom_labels = pd.read_csv(secom_labels_url, sep=\" \", header=None)\n",
        "\n",
        "# Preprocessing: handle missing values and scale data\n",
        "secom_df.replace('NaN', np.nan, inplace=True)\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "secom_df_imputed = imputer.fit_transform(secom_df)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "secom_df_scaled = scaler.fit_transform(secom_df_imputed)\n",
        "\n",
        "# Convert labels to binary\n",
        "secom_labels.columns = ['Result', 'Timestamp']\n",
        "secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "# Step 1: Apply PCA for feature reduction\n",
        "pca = PCA(n_components=100)  # Reduce to 100 components for now\n",
        "secom_df_reduced = pca.fit_transform(secom_df_scaled)\n",
        "\n",
        "# Step 2: Apply SMOTE for class balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(secom_df_reduced, secom_labels['Result'])\n",
        "\n",
        "# Train-test split with balanced data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors for both features and labels\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define a custom dataset\n",
        "class SECOMDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define Transformer Model for classification with Dropout\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout),\n",
        "            num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)  # Apply dropout after embedding\n",
        "        x = x.unsqueeze(0)  # Add batch dimension for transformer input\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # Remove batch dimension\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, criterion, optimizer and scheduler\n",
        "model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Function to train the model with scheduler\n",
        "def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler after each epoch based on the running loss\n",
        "        scheduler.step(running_loss / len(train_loader))\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Train the model\n",
        "train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "y_true, y_pred = evaluate_model(model, test_loader)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXTt-U79pPHj",
        "outputId": "21e0cd91-9983-45bd-c81f-36bdaa643511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.7831966208445059\n",
            "Epoch 2/20, Loss: 0.5095763150099162\n",
            "Epoch 3/20, Loss: 0.4603849321201041\n",
            "Epoch 4/20, Loss: 0.42416659057945816\n",
            "Epoch 5/20, Loss: 0.38424061601226395\n",
            "Epoch 6/20, Loss: 0.34833383182617456\n",
            "Epoch 7/20, Loss: 0.27861903310828917\n",
            "Epoch 8/20, Loss: 0.2965499191872171\n",
            "Epoch 9/20, Loss: 0.28866263176943807\n",
            "Epoch 10/20, Loss: 0.2989032208114057\n",
            "Epoch 11/20, Loss: 0.2449187186320086\n",
            "Epoch 12/20, Loss: 0.2861582774266198\n",
            "Epoch 13/20, Loss: 0.2734248718699893\n",
            "Epoch 14/20, Loss: 0.27398209726891004\n",
            "Epoch 15/20, Loss: 0.3091446297192896\n",
            "Epoch 16/20, Loss: 0.324730424260771\n",
            "Epoch 17/20, Loss: 0.24699894157615868\n",
            "Epoch 18/20, Loss: 0.21663895888707121\n",
            "Epoch 19/20, Loss: 0.2076412876312797\n",
            "Epoch 20/20, Loss: 0.18402271133822365\n",
            "Accuracy: 0.9505119453924915\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.97      0.95       280\n",
            "           1       0.98      0.93      0.95       306\n",
            "\n",
            "    accuracy                           0.95       586\n",
            "   macro avg       0.95      0.95      0.95       586\n",
            "weighted avg       0.95      0.95      0.95       586\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Load the SECOM dataset\n",
        "secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
        "secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
        "\n",
        "# Load features and labels\n",
        "secom_df = pd.read_csv(secom_data_url, sep=\" \", header=None)\n",
        "secom_labels = pd.read_csv(secom_labels_url, sep=\" \", header=None)\n",
        "\n",
        "# Preprocessing: handle missing values and scale data\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "secom_df_imputed = imputer.fit_transform(secom_df)\n",
        "scaler = StandardScaler()\n",
        "secom_df_scaled = scaler.fit_transform(secom_df_imputed)\n",
        "\n",
        "# Convert labels to binary\n",
        "secom_labels.columns = ['Result', 'Timestamp']\n",
        "secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "# Step 1: Apply PCA for feature reduction\n",
        "pca = PCA(n_components=100)\n",
        "secom_df_reduced = pca.fit_transform(secom_df_scaled)\n",
        "\n",
        "# Step 2: Apply SMOTE for class balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(secom_df_reduced, secom_labels['Result'])\n",
        "\n",
        "# Train-test split with balanced data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors for both features and labels\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define a custom dataset\n",
        "class SECOMDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define Transformer Model for classification with Dropout\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)    # Apply dropout after embedding\n",
        "\n",
        "        # Reshape for transformer: (batch_size, seq_len (1), hidden_dim)\n",
        "        x = x.unsqueeze(1)      # Add sequence length dimension\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)         # Shape: (batch_size, seq_len (1), hidden_dim)\n",
        "\n",
        "        x = x.squeeze(1)         # Remove sequence length dimension\n",
        "        x = self.fc(x)           # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize model with Dropout and optimizer\n",
        "model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Function to train the model with scheduler\n",
        "def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler after each epoch based on the running loss\n",
        "        scheduler.step(running_loss / len(train_loader))\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Train the model with increased epochs and improved structure\n",
        "train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
        "\n",
        "# Evaluate the model and print results\n",
        "y_true, y_pred = evaluate_model(model, test_loader)\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4eBRQ97qOMd",
        "outputId": "6bb6395f-0c1a-4bc7-dde4-289a2a1ed4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.49915729422827027\n",
            "Epoch 2/20, Loss: 0.2744014007920349\n",
            "Epoch 3/20, Loss: 0.2578466400202062\n",
            "Epoch 4/20, Loss: 0.3891297385499284\n",
            "Epoch 5/20, Loss: 0.29761497062203046\n",
            "Epoch 6/20, Loss: 0.38679545231767604\n",
            "Epoch 7/20, Loss: 0.32348049683748065\n",
            "Epoch 8/20, Loss: 0.3150554259081145\n",
            "Epoch 9/20, Loss: 0.2743542015149787\n",
            "Epoch 10/20, Loss: 0.23878937350535714\n",
            "Epoch 11/20, Loss: 0.22856573508800687\n",
            "Epoch 12/20, Loss: 0.21227628304748922\n",
            "Epoch 13/20, Loss: 0.18278607917395798\n",
            "Epoch 14/20, Loss: 0.17113674119920344\n",
            "Epoch 15/20, Loss: 0.16065678433389277\n",
            "Epoch 16/20, Loss: 0.15169508299614126\n",
            "Epoch 17/20, Loss: 0.13662615712933443\n",
            "Epoch 18/20, Loss: 0.12366162214075795\n",
            "Epoch 19/20, Loss: 0.13835279768131473\n",
            "Epoch 20/20, Loss: 0.10786745460653627\n",
            "Accuracy: 0.9419795221843004\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94       280\n",
            "           1       1.00      0.89      0.94       306\n",
            "\n",
            "    accuracy                           0.94       586\n",
            "   macro avg       0.95      0.94      0.94       586\n",
            "weighted avg       0.95      0.94      0.94       586\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Load the SECOM dataset\n",
        "secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
        "secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
        "\n",
        "# Load features and labels\n",
        "secom_df = pd.read_csv(secom_data_url, sep=\" \", header=None)\n",
        "secom_labels = pd.read_csv(secom_labels_url, sep=\" \", header=None)\n",
        "\n",
        "# Preprocessing: handle missing values and scale data\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "secom_df_imputed = imputer.fit_transform(secom_df)\n",
        "scaler = StandardScaler()\n",
        "secom_df_scaled = scaler.fit_transform(secom_df_imputed)\n",
        "\n",
        "# Convert labels to binary\n",
        "secom_labels.columns = ['Result', 'Timestamp']\n",
        "secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "# Apply SMOTE for class balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(secom_df_scaled, secom_labels['Result'])\n",
        "\n",
        "# Train-test split with balanced data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors for both features and labels\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define a custom dataset\n",
        "class SECOMDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define Transformer Model for classification with Dropout and Batch Normalization\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)    # Apply dropout after embedding\n",
        "\n",
        "        # Reshape for transformer: (batch_size, seq_len (1), hidden_dim)\n",
        "        x = x.unsqueeze(1)      # Add sequence length dimension\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)         # Shape: (batch_size, seq_len (1), hidden_dim)\n",
        "\n",
        "        x = x.squeeze(1)         # Remove sequence length dimension\n",
        "        x = self.fc(x)           # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize model with Dropout and optimizer\n",
        "model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Function to train the model with scheduler\n",
        "def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler after each epoch based on the running loss\n",
        "        scheduler.step(running_loss / len(train_loader))\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Train the model with increased epochs and improved structure\n",
        "train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
        "\n",
        "# Evaluate the model and print results\n",
        "y_true, y_pred = evaluate_model(model, test_loader)\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Load the SECOM dataset\n",
        "secom_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
        "secom_labels_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
        "\n",
        "# Load features and labels\n",
        "secom_df = pd.read_csv(secom_data_url, sep=\" \", header=None)\n",
        "secom_labels = pd.read_csv(secom_labels_url, sep=\" \", header=None)\n",
        "\n",
        "# Preprocessing: handle missing values and scale data\n",
        "secom_df.replace('NaN', np.nan, inplace=True)\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "secom_df_imputed = imputer.fit_transform(secom_df)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "secom_df_scaled = scaler.fit_transform(secom_df_imputed)\n",
        "\n",
        "# Convert labels to binary\n",
        "secom_labels.columns = ['Result', 'Timestamp']\n",
        "secom_labels['Result'] = secom_labels['Result'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "# Step 1: Apply SMOTE for class balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(secom_df_scaled, secom_labels['Result'])\n",
        "\n",
        "# Step 2: Train-test split with balanced data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors for both features and labels\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define a custom dataset\n",
        "class SECOMDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = SECOMDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = SECOMDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define Transformer Model for classification with Dropout and Batch Normalization\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, nhead=8, num_layers=3, dropout=0.3):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)    # Apply dropout after embedding\n",
        "\n",
        "        # Reshape for transformer: (batch_size, seq_len (1), hidden_dim)\n",
        "        x = x.unsqueeze(1)      # Add sequence length dimension\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)         # Shape: (batch_size, seq_len (1), hidden_dim)\n",
        "\n",
        "        x = x.squeeze(1)         # Remove sequence length dimension\n",
        "        x = self.fc(x)           # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize model with Dropout and optimizer\n",
        "model = TransformerModel(input_dim=X_train.shape[1], num_classes=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Function to train the model with scheduler\n",
        "def train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler after each epoch based on the running loss\n",
        "        scheduler.step(running_loss / len(train_loader))\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Train the model with increased epochs and improved structure\n",
        "train_model_with_scheduler(model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
        "\n",
        "# Evaluate the model and print results\n",
        "y_true_pca , y_pred_pca  = evaluate_model(model,test_loader)\n",
        "print(f\"Accuracy after PCA: {accuracy_score(y_true_pca , y_pred_pca )}\")\n",
        "print(classification_report(y_true_pca , y_pred_pca ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGvEVt-Wu2vH",
        "outputId": "e529c3fc-3e89-45c1-8777-9c72bd328448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.5021539733216569\n",
            "Epoch 2/20, Loss: 0.28203576707558053\n",
            "Epoch 3/20, Loss: 0.2593970114012828\n",
            "Epoch 4/20, Loss: 0.2455261353201963\n",
            "Epoch 5/20, Loss: 0.2585328446348777\n",
            "Epoch 6/20, Loss: 0.26943880709743984\n",
            "Epoch 7/20, Loss: 0.32298274642813046\n",
            "Epoch 8/20, Loss: 0.30910241966311996\n",
            "Epoch 9/20, Loss: 0.22280928552956195\n",
            "Epoch 10/20, Loss: 0.19848586336986437\n",
            "Epoch 11/20, Loss: 0.16284939108064045\n",
            "Epoch 12/20, Loss: 0.1596764937244557\n",
            "Epoch 13/20, Loss: 0.14716129203805248\n",
            "Epoch 14/20, Loss: 0.12157320224906544\n",
            "Epoch 15/20, Loss: 0.12290389744590062\n",
            "Epoch 16/20, Loss: 0.09367998700739967\n",
            "Epoch 17/20, Loss: 0.09350105815541905\n",
            "Epoch 18/20, Loss: 0.09122443158884307\n",
            "Epoch 19/20, Loss: 0.08471216303548096\n",
            "Epoch 20/20, Loss: 0.07160097521075325\n",
            "Accuracy after PCA: 0.9453924914675768\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95       280\n",
            "           1       1.00      0.90      0.95       306\n",
            "\n",
            "    accuracy                           0.95       586\n",
            "   macro avg       0.95      0.95      0.95       586\n",
            "weighted avg       0.95      0.95      0.95       586\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}